# ğŸ”¥ LLMDump

```
_     _     __  __ ____
 | |   | |   |  \/  |  _ \ _   _ _ __ ___  _ __
 | |   | |   | |\/| | | | | | | | '_ ` _ \| '_ \
 | |___| |___| |  | | |_| | |_| | | | | | | |_) |
 |_____|_____|_|  |_|____/ \__,_|_| |_| |_| .__/
                                          |_|
```

> Automatically crawl documentation, clean up extra markup, and write to markdown for use with LLMs

## ğŸš€ Features

- ğŸ” Crawl documentation sites with configurable depth
- ğŸ§¹ Clean up extra markup and formatting
- ğŸ“š Automatically categorize content
- ğŸ“ Generate clean markdown files
- ğŸ’¾ Archive and manage multiple crawls
- ğŸ”„ Interactive document pruning and category splitting
- ğŸ“Š Token estimation for LLM context windows

## ğŸ“¦ Installation

```bash
npm install -g llmdump
```

## ğŸ”‘ Setup

You'll need two API keys:

1. Firecrawl API key for web crawling
2. OpenAI API key for content processing

Set them as environment variables or provide them via CLI:

```bash
export FIRECRAWL_API_KEY="your-firecrawl-key"
export OPENAI_API_KEY="your-openai-key"
```

Or use CLI flags:

```bash
llmdump --firecrawl-key "your-key" --openai-key "your-key"
```

## ğŸ¯ Usage

### Basic Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Start Crawl   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  View Summary   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Clean & Save   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Archive      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Examples

1. Start a new crawl:

```bash
llmdump
# Follow interactive prompts to enter URL and crawl depth
```

2. Continue from existing crawl:

```bash
llmdump
# Select "Continue from [crawl-id]" from main menu
```

3. View archived crawls:

```bash
llmdump
# Select "View Archived Crawls" from main menu
```

### Directory Structure

```
.data/
â”œâ”€â”€ current-crawl/          # Active crawl data
â”‚   â”œâ”€â”€ crawlResult.json   # Raw crawl results
â”‚   â”œâ”€â”€ identifier.json    # Unique crawl identifier
â”‚   â”œâ”€â”€ categories.json    # Content categorization
â”‚   â””â”€â”€ output/           # Generated markdown files
â””â”€â”€ history/              # Archived crawls
    â””â”€â”€ [crawl-id]-[timestamp]/
```

## ğŸ› ï¸ Processing Options

### View Documents Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Documents: 25                   â”‚
â”‚ Categories: 5                   â”‚
â”‚ Total Tokens: ~150,000         â”‚
â”‚                                 â”‚
â”‚ Category: Getting Started (8)   â”‚
â”‚ Category: API Reference (12)    â”‚
â”‚ Category: Examples (5)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Clean & Concat Options

1. Single file (all content)
2. Multiple files (one per category)

### Document Management

- Prune irrelevant content
- Split categories
- Archive crawls
- View and edit categories

## ğŸ” Example Workflow

```bash
# 1. Start new crawl
llmdump
> Enter URL: https://docs.example.com
> Enter limit: 20

# 2. View summary
> Select "View Documents Summary"

# 3. Process documents
> Select "Clean & Concat Documents"
> Choose output format (single/multiple)

# 4. Archive
> Select "Archive This Crawl"
```

## ğŸ“ Output Format

### Single File Output

```markdown
# Getting Started

## Introduction

[Content...]

# API Reference

## Endpoints

[Content...]
```

### Multiple Files Output

```
output/
â”œâ”€â”€ getting-started.md
â”œâ”€â”€ api-reference.md
â””â”€â”€ examples.md
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

MIT
